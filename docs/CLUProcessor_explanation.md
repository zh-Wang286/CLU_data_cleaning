# `CLUProcessor` 技术说明

## 1. 背景

在对话式AI项目中，训练数据的质量是决定模型能否准确理解用户意图的关键。`CLUProcessor`工具作为数据清洗流程的核心模块，负责对初步加载和校验过的数据集进行深度分析。

## 2. 目的

`CLUProcessor` 的核心目标是**量化**和**定位**训练数据中的潜在问题。它不依赖于人类的直观感受，而是通过数学和算法模型来回答以下几个关键问题：

-   **意图内聚性**: 单个意图下的所有语料，在语义上是否足够收敛？是否存在一些语义漂移的异常样本？
-   **意图耦合性**: 不同意图之间的语义边界是否清晰？是否存在定义重叠、容易让模型混淆的灰色地带？
-   **数据充分性**: 是否有某些意图的样本量过少，可能导致模型欠拟合？

## 3. 采用的主要方法

`CLUProcessor`的分析流程主要分为以下几个步骤：

### 3.1. 文本向量化 (Text Embedding)

这是所有后续分析的基石。通过调用Azure OpenAI的`text-embedding-3-large`模型，将每一句自然语言（utterance）映射到一个高维（3072维）的向量空间中。

这么做的原因是，一旦文本被表示为向量，就可以用数学方式来度量它们之间的语义关系。在向量空间中，语义相似的句子，其向量方向也更接近。这本质上是将一个复杂的NLP问题，转化成了一个可以精确计算的几何问题。

在具体实现上，考虑了效率和稳定性：
-   **缓存机制**: 为了节约API调用成本和时间，计算出的向量会基于输入语料的哈希值进行本地缓存。
-   **批量处理**: 将多个句子打包成一批进行请求，提升网络效率。

### 3.2. 意图内异常点检测 (Intra-Intent Outlier Detection)

这个方法用来评估“意图内聚性”，逐一检查每个意图，找出该意图内部在语义上“离群”的语料。

一个意图下的所有句子理应围绕一个核心语义。如果某个句子的向量与其他所有句子的向量在空间中都相距甚远，那么它很可能是一个异常点（例如，标注错误）。

此处采用了一种非参数的方法（k-NN）来实现：
1.  在一个意图内，计算每个语料向量到其他所有向量的**余弦距离**。
2.  将每个点到其最近邻（k=1）的距离，定义为该点的**异常分数**。分数越高，代表它越“孤立”。
3.  为了避免硬编码一个固定的阈值，采用统计学方法，将该意图内所有异常分数的**95百分位**作为动态阈值。
4.  所有异常分数超过这个阈值的语料，就被判定为离群点。

### 3.3. 全局聚类审计 (Global Clustering Audit)

这是一种无监督的审查方式，用来评估“意图耦合性”。该方法暂时忽略所有语料原有的意图标签，将整个数据集的所有向量放在一起，让聚类算法根据语义相似性自由地将它们分成若干个簇。

理想情况下，如果数据质量好，算法形成的每个簇应该主要由来自同一个原始意图的语料构成。如果一个簇里高度混合了来自“意图A”和“意图B”的语料，这就强烈暗示这两个意图的定义可能存在重叠。

选择的算法是**HDBSCAN**，因为它是一种强大的密度聚类算法，有两个关键优点：它能自动确定簇的数量，并且能将不属于任何一个簇的“噪声点”识别出来。

### 3.4. 意图边界混淆分析 (Boundary Violation Analysis)

聚类审计提供了一个宏观的视图，而这个方法则提供了一种更精细、更具统计意义的方式来审查意图边界。它会逐一检查每个语料，并计算它在统计上有多大概率可以被归属到另一个意图。

这个分析的目标是精确找出那些“身在曹营心在汉”的语料。

具体实现上，采用了PCA和马氏距离的组合：
1.  **处理“维度灾难” (PCA)**：直接在高维空间（3072维）中对小样本数据进行统计建模是极不稳定的。因此，首先使用**主成分分析 (PCA)** 将所有向量投影到一个低维空间。在选择降维维度时，通过计算对比发现，传统的“保留95%方差”策略会导致维度过高，反而引入噪声，降低了意图的可区分性。最终选择了一个更低的维度，该维度虽然只捕获了部分方差，但在量化评估中（通过“意图可分离性分数”），它最大化了不同意图簇之间的边界，并确保了后续马氏距离计算的数值稳定性。这是一种通过实验验证的、以任务为导向的去噪策略。
2.  **构建概率分布**: 在降维后的空间里，为每个意图的语料分布计算其中心点（均值向量）和形状（协方差矩阵）。
3.  **衡量归属可能性 (马氏距离)**：对于每个语料，计算它到**其他所有意图**分布中心的**马氏距离**。与简单的欧氏距离不同，马氏距离考虑了数据分布的形状（协方差），能更准确地判断一个点是否是某个分布中的“典型”成员。
4.  **转换为概率 (p-value)**：最后，将马氏距离通过卡方分布转换为一个p-value。这个值可以通俗地理解为：“假如我们认定这个语料属于目标意图，那么这个认定的合理性有多高”。一个高的p-value（例如 > 0.05）强烈暗示这个语料可以被合理地看作是另一个意图的成员。

### 3.5. 数据增广 (Data Enrichment)

这是一个辅助功能，用于解决“数据充分性”问题。对于那些样本量过少的意图，设计了一个流程，可以调用大语言模型（LLM）来生成新的、语义一致的候选语料。

实现上，会构建一个详细的指令（Prompt），其中包含意图的定义、已有的语料作为few-shot范例，以及对生成多样性的明确指导。需要强调的是，这个功能是半自动的，它只会生成候选语料列表供人工审核，并不会自动将其加入数据集。

## 4. 输出产物及解读

### 4.1. 完整分析 (`run-all` 命令)

运行 `run-all` 命令后，会在 `outputs/` 目录下看到以下产物：

-   **审计报告 (位于 `outputs/reports/`)**:
    -   这是一个带时间戳的Markdown文件，是需要最优先关注的核心产物，设计为对所有分析结果的汇总：
        -   **数据集概览**: 提供了意图和语料总数，并列出了样本量过少的意图。
        -   **意图内部异常点**: 分意图列出所有被识别为异常点的语料。**这里列出的句子，建议重点审查，判断是否存在标注错误。**
        -   **全局聚类审计**: 列出所有算法发现的簇，并报告每个簇的“纯度”（即主要意图的占比）。**需要重点关注那些“纯度”较低的簇，其中混合的意- 意图可能存在定义重叠。**
        -   **意图边界混淆分析**: 这是对聚类审计的深化。它会精确列出每一个被判定为“边界模糊”的语料和它最可能混淆的目标意图，并附上p-value。**这份列表是进行数据修正时优先级最高的参考。**

-   **可视化图表 (位于 `outputs/figures/<运行时间>/`)**:
    -   **意图相似度热力图**: 通过计算每个意图的平均向量，然后计算它们之间的余弦相似度来生成此图。颜色越亮，表示两个意图在语义上越接近。
    -   **全局语料散点图**: 这是所有语料在2D空间中的分布图。图中用**黄边三角形**特别标出了被“意图边界混淆分析”识别出的高风险语料，可以宏观地定位意图重叠区域。
    -   **意图内语料散点图**: 每个意图都有一张单独的图，直观地展示了正常点和被判定的异常点，方便理解某个点被判定为异常的原因。

### 4.2. 数据增广 (`enrich` 命令)

运行这个命令后，默认会生成一份独立的Markdown报告，其中包含了为每个低样本意图生成的所有候选语料。**在人工审核这些语料后，再决定是否将它们加入原始数据集。** 如果只想在控制台快速查看结果，可以使用`--dry-run`标志。
