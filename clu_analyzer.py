#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Azure CLUæ•°æ®è´¨é‡åˆ†æå™¨

æœ¬æ¨¡å—å®ç°å¯¹Azure CLUè®­ç»ƒæ•°æ®çš„è¯­ä¹‰åˆ†æï¼ŒåŒ…æ‹¬ï¼š
1. Intenté—´ä½è€¦åˆæ€§åˆ†æï¼ˆè¯­ä¹‰åŒºåˆ†åº¦ï¼‰
2. Intentå†…é«˜å†…èšæ€§åˆ†æï¼ˆæ ·æœ¬ä¸€è‡´æ€§ï¼‰
3. æ•°æ®å¯è§†åŒ–ä¸è´¨é‡æŠ¥å‘Š

ä½œè€…: Data Science Team
åˆ›å»ºæ—¶é—´: 2025-01-27
"""

import json
import logging
import os
import warnings
from collections import defaultdict
from typing import Dict, List, Tuple, Any, Optional
from concurrent.futures import ThreadPoolExecutor

import hdbscan
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from openai import AzureOpenAI
from sklearn.metrics import silhouette_score
from sklearn.metrics.pairwise import cosine_similarity
from umap import UMAP
from mpl_toolkits.mplot3d import Axes3D

# å¿½ç•¥ç‰¹å®šçš„FutureWarning
warnings.filterwarnings("ignore", category=FutureWarning, module='sklearn.utils.deprecation')
warnings.filterwarnings("ignore", category=UserWarning, module='umap.umap_')

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class CLUDataAnalyzer:
    """Azure CLUæ•°æ®è´¨é‡åˆ†æå™¨ä¸»ç±»"""
    
    def __init__(self, 
                 api_key: str,
                 azure_endpoint: str,
                 deployment_name: str = "NNIT-Ada-3-large",
                 api_version: str = "2024-12-01-preview"):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            api_key: Azure OpenAI APIå¯†é’¥
            azure_endpoint: Azure OpenAIæœåŠ¡ç«¯ç‚¹
            deployment_name: éƒ¨ç½²æ¨¡å‹åç§°
            api_version: APIç‰ˆæœ¬
        """
        self.client = AzureOpenAI(
            api_key=api_key,
            azure_endpoint=azure_endpoint,
            api_version=api_version
        )
        self.deployment_name = deployment_name
        
        # æ•°æ®å­˜å‚¨
        self.intents: List[str] = []
        self.utterances: List[Dict[str, Any]] = []
        self.intent_utterances: Dict[str, List[str]] = defaultdict(list)
        self.text_to_embedding: Dict[str, np.ndarray] = {}
        
        logger.info(f"CLUåˆ†æå™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {deployment_name}")
    
    def load_data(self, json_file_path: str) -> None:
        """
        åŠ è½½CLUè®­ç»ƒæ•°æ®
        
        Args:
            json_file_path: JSONæ•°æ®æ–‡ä»¶è·¯å¾„
        """
        logger.info(f"æ­£åœ¨åŠ è½½æ•°æ®æ–‡ä»¶: {json_file_path}")
        
        try:
            with open(json_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # æå–Intentåˆ—è¡¨
            self.intents = [intent['category'] for intent in data['assets']['intents']]
            
            # æå–utterances
            self.utterances = data['assets']['utterances']
            
            # æŒ‰Intentåˆ†ç»„utterances
            for utterance in self.utterances:
                intent = utterance['intent']
                text = utterance['text']
                self.intent_utterances[intent].append(text)
            
            logger.info(f"æ•°æ®åŠ è½½å®Œæˆ: {len(self.intents)}ä¸ªIntent, {len(self.utterances)}æ¡utterance")
            
        except Exception as e:
            logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            raise
    
    def _generate_all_embeddings(self):
        """ä¸€æ¬¡æ€§ä¸ºæ‰€æœ‰å”¯ä¸€çš„utteranceç”ŸæˆåµŒå…¥å‘é‡ï¼Œä»¥æé«˜æ•ˆç‡ã€‚"""
        logger.info("å¼€å§‹ä¸ºæ‰€æœ‰utterancesç”ŸæˆåµŒå…¥å‘é‡...")
        all_texts = [u['text'] for u in self.utterances]
        
        unique_texts = list(set(all_texts))
        logger.info(f"æ€»å…± {len(all_texts)} utterances, å…¶ä¸­ {len(unique_texts)} æ˜¯å”¯ä¸€çš„ã€‚")
        
        if not unique_texts:
            logger.warning("æ•°æ®ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•utteranceæ–‡æœ¬ã€‚")
            return

        unique_embeddings_array = self.get_embeddings(unique_texts)
        
        self.text_to_embedding = {text: emb for text, emb in zip(unique_texts, unique_embeddings_array)}
        
        logger.info("æ‰€æœ‰å”¯ä¸€utterancesçš„åµŒå…¥å‘é‡å·²ç”Ÿæˆã€‚")

    def get_embeddings(self, texts: List[str], batch_size: int = 100, max_workers: int = 20) -> np.ndarray:
        """
        æ‰¹é‡è·å–æ–‡æœ¬åµŒå…¥å‘é‡ï¼ˆå¤šçº¿ç¨‹ï¼‰
        
        Args:
            texts: å¾…åµŒå…¥çš„æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            max_workers: å¹¶å‘çº¿ç¨‹æ•°
            
        Returns:
            åµŒå…¥å‘é‡çŸ©é˜µ
        """
        logger.info(f"æ­£åœ¨è·å–{len(texts)}ä¸ªæ–‡æœ¬çš„åµŒå…¥å‘é‡ (ä½¿ç”¨{max_workers}ä¸ªçº¿ç¨‹)")
        
        batches = [texts[i:i + batch_size] for i in range(0, len(texts), batch_size)]
        
        # ä½¿ç”¨å­—å…¸ç¡®ä¿åµŒå…¥å‘é‡æŒ‰åŸå§‹é¡ºåºæ’åˆ—
        embeddings_dict: Dict[int, List[List[float]]] = {}

        def get_batch_embedding(batch_texts: List[str], batch_index: int):
            """è·å–å•ä¸ªæ‰¹æ¬¡çš„åµŒå…¥å‘é‡"""
            try:
                response = self.client.embeddings.create(
                    input=batch_texts,
                    model=self.deployment_name
                )
                embeddings_dict[batch_index] = [data.embedding for data in response.data]
                logger.info(f"æ‰¹æ¬¡ {batch_index + 1}/{len(batches)} å¤„ç†å®Œæˆ")
            except Exception as e:
                logger.error(f"æ‰¹æ¬¡ {batch_index} åµŒå…¥å‘é‡è·å–å¤±è´¥: {str(e)}")
                # æ ‡è®°å¤±è´¥ï¼Œä½†ä¸å­˜å‚¨ä»»ä½•å†…å®¹
                embeddings_dict[batch_index] = []

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰æ‰¹æ¬¡ä»»åŠ¡
            futures = [executor.submit(get_batch_embedding, batch, i) for i, batch in enumerate(batches)]
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            for future in futures:
                future.result() # ç­‰å¾…ä»»åŠ¡å®Œæˆï¼Œå¹¶æ•è·å¼‚å¸¸
        
        # æŒ‰æ‰¹æ¬¡ç´¢å¼•æ’åºå¹¶åˆå¹¶ç»“æœ
        full_embeddings = []
        for i in range(len(batches)):
            batch_result = embeddings_dict.get(i)
            if batch_result is None or not batch_result:
                # å¦‚æœæŸä¸ªæ‰¹æ¬¡å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
                raise RuntimeError(f"æ‰¹æ¬¡ {i} çš„åµŒå…¥å‘é‡è·å–å¤±è´¥ï¼Œåˆ†æä¸­æ­¢")
            full_embeddings.extend(batch_result)
        
        return np.array(full_embeddings)
    
    def analyze_inter_intent_coupling(self) -> Tuple[np.ndarray, pd.DataFrame, List[str]]:
        """
        åˆ†æIntenté—´çš„ä½è€¦åˆæ€§ï¼ˆè¯­ä¹‰åŒºåˆ†åº¦ï¼‰
        
        Returns:
            ç›¸ä¼¼åº¦çŸ©é˜µ, åˆ†æç»“æœDataFrame, å’Œç”¨äºç”ŸæˆçŸ©é˜µçš„intentåç§°åˆ—è¡¨
        """
        logger.info("å¼€å§‹Intenté—´ä½è€¦åˆæ€§åˆ†æ")
        
        # è®¡ç®—æ¯ä¸ªIntentçš„ä»£è¡¨æ€§åµŒå…¥ï¼ˆå¹³å‡å€¼ï¼‰
        intent_embeddings = []
        ordered_intent_names = []
        
        for intent in self.intents:
            if intent in self.intent_utterances and self.intent_utterances[intent]:
                utterance_texts = self.intent_utterances[intent]
                embeddings = np.array([self.text_to_embedding[text] for text in utterance_texts])
                
                # ä½¿ç”¨å¹³å‡åµŒå…¥ä½œä¸ºIntentçš„ä»£è¡¨
                intent_embedding = np.mean(embeddings, axis=0)
                intent_embeddings.append(intent_embedding)
                ordered_intent_names.append(intent)
        
        intent_embeddings = np.array(intent_embeddings)
        
        # è®¡ç®—Intenté—´ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = cosine_similarity(intent_embeddings)
        
        # åˆ†æé«˜ç›¸ä¼¼åº¦çš„Intentå¯¹
        high_similarity_pairs = []
        threshold = 0.8  # é«˜ç›¸ä¼¼åº¦é˜ˆå€¼
        
        for i in range(len(ordered_intent_names)):
            for j in range(i + 1, len(ordered_intent_names)):
                similarity = similarity_matrix[i][j]
                if similarity > threshold:
                    high_similarity_pairs.append({
                        'Intent1': ordered_intent_names[i],
                        'Intent2': ordered_intent_names[j],
                        'Similarity': similarity
                    })
        
        results_df = pd.DataFrame(high_similarity_pairs)
        results_df = results_df.sort_values('Similarity', ascending=False)
        
        logger.info(f"å‘ç°{len(high_similarity_pairs)}å¯¹é«˜ç›¸ä¼¼åº¦Intent")
        
        return similarity_matrix, results_df, ordered_intent_names
    
    def analyze_intra_intent_cohesion(self, min_cluster_size: int = 3) -> Dict[str, Dict]:
        """
        åˆ†æIntentå†…çš„é«˜å†…èšæ€§ï¼ˆæ ·æœ¬ä¸€è‡´æ€§ï¼‰
        
        Args:
            min_cluster_size: HDBSCANæœ€å°èšç±»å¤§å°
            
        Returns:
            æ¯ä¸ªIntentçš„èšç±»åˆ†æç»“æœ
        """
        logger.info("å¼€å§‹Intentå†…é«˜å†…èšæ€§åˆ†æ")
        
        cohesion_results = {}
        
        for intent in self.intents:
            if intent not in self.intent_utterances or len(self.intent_utterances[intent]) < min_cluster_size:
                continue
            
            logger.info(f"åˆ†æIntent: {intent}")
            
            utterance_texts = self.intent_utterances[intent]
            embeddings = np.array([self.text_to_embedding[text] for text in utterance_texts])
            
            # ä½¿ç”¨HDBSCANè¿›è¡Œèšç±»
            clusterer = hdbscan.HDBSCAN(
                min_cluster_size=min_cluster_size,
                metric='euclidean' # OpenAIåµŒå…¥å‘é‡å·²å½’ä¸€åŒ–, ä½¿ç”¨æ¬§æ°è·ç¦»ç­‰ä»·äºä½™å¼¦ç›¸ä¼¼åº¦
            )
            
            cluster_labels = clusterer.fit_predict(embeddings)
            
            # è®¡ç®—èšç±»è´¨é‡æŒ‡æ ‡
            n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
            n_noise = list(cluster_labels).count(-1)
            
            # è®¡ç®—è½®å»“ç³»æ•°ï¼ˆå¦‚æœæœ‰å¤šä¸ªèšç±»ï¼‰
            silhouette_avg = 0.0
            if n_clusters > 1:
                # è¿‡æ»¤å™ªå£°ç‚¹
                valid_indices = cluster_labels != -1
                if np.sum(valid_indices) > 1:
                    silhouette_avg = silhouette_score(
                        embeddings[valid_indices], 
                        cluster_labels[valid_indices],
                        metric='cosine'
                    )
            
            cohesion_results[intent] = {
                'n_samples': len(utterance_texts),
                'n_clusters': n_clusters,
                'n_noise': n_noise,
                'noise_ratio': n_noise / len(utterance_texts),
                'silhouette_score': silhouette_avg,
                'cluster_labels': cluster_labels,
                'utterances': utterance_texts,
                'embeddings': embeddings
            }
        
        logger.info(f"å®Œæˆ{len(cohesion_results)}ä¸ªIntentçš„å†…èšæ€§åˆ†æ")
        
        return cohesion_results
    
    def create_visualizations(self, 
                            similarity_matrix: np.ndarray,
                            intent_names: List[str],
                            cohesion_results: Dict[str, Dict],
                            output_dir: str = "output") -> None:
        """
        åˆ›å»ºæ•°æ®å¯è§†åŒ–å›¾è¡¨
        
        Args:
            similarity_matrix: Intenté—´ç›¸ä¼¼åº¦çŸ©é˜µ
            intent_names: Intentåç§°åˆ—è¡¨
            cohesion_results: å†…èšæ€§åˆ†æç»“æœ
            output_dir: è¾“å‡ºç›®å½•
        """
        logger.info("å¼€å§‹åˆ›å»ºå¯è§†åŒ–å›¾è¡¨")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # å¼ºåˆ¶é‡å»ºmatplotlibå­—ä½“ç¼“å­˜ä»¥è¯†åˆ«æ–°å®‰è£…çš„å­—ä½“
        try:
            import matplotlib
            cache_dir = matplotlib.get_cachedir()
            for file in os.listdir(cache_dir):
                if file.startswith('fontlist-') and file.endswith('.json'):
                    os.remove(os.path.join(cache_dir, file))
            logger.info("Matplotlibå­—ä½“ç¼“å­˜å·²æ¸…é™¤ï¼Œå°†è¿›è¡Œé‡å»ºã€‚")
        except Exception as e:
            logger.warning(f"æ— æ³•æ¸…é™¤Matplotlibå­—ä½“ç¼“å­˜: {e}")

        # åŠ¨æ€è®¾ç½®æ”¯æŒä¸­æ–‡çš„å­—ä½“
        try:
            from matplotlib.font_manager import FontProperties
            
            # å¸¸è§çš„æ”¯æŒä¸­æ–‡çš„å­—ä½“åˆ—è¡¨
            font_list = ['WenQuanYi Micro Hei', 'SimHei', 'WenQuanYi Zen Hei', 'Microsoft YaHei', 'Heiti TC', 'sans-serif']
            
            # æŸ¥æ‰¾å¹¶è®¾ç½®å¯ç”¨å­—ä½“
            font_path = None
            for font in font_list:
                try:
                    # å°è¯•æŸ¥æ‰¾å­—ä½“
                    font_path = FontProperties(font).get_name()
                    plt.rcParams['font.sans-serif'] = [font]
                    logger.info(f"æˆåŠŸè®¾ç½®ä¸­æ–‡å­—ä½“: {font}")
                    break
                except:
                    continue
            
            if not font_path:
                 logger.warning("æœªæ‰¾åˆ°å¯ç”¨çš„ä¸­æ–‡å­—ä½“ï¼Œå›¾è¡¨ä¸­çš„ä¸­æ–‡å¯èƒ½æ— æ³•æ­£å¸¸æ˜¾ç¤ºã€‚è¯·å®‰è£…'SimHei'æˆ–'WenQuanYi Zen Hei'ç­‰å­—ä½“ã€‚")

        except ImportError:
            logger.warning("matplotlib.font_manageræœªæ‰¾åˆ°ï¼Œæ— æ³•åŠ¨æ€è®¾ç½®ä¸­æ–‡å­—ä½“ã€‚")

        plt.rcParams['axes.unicode_minus'] = False # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
        
        # 1. Intentç›¸ä¼¼åº¦çƒ­åŠ›å›¾
        plt.figure(figsize=(20, 16))
        
        # åªæ˜¾ç¤ºIntentåç§°çš„ååŠéƒ¨åˆ†ï¼ˆå»æ‰åˆ†ç±»å‰ç¼€ï¼‰
        display_names = [name.split('_')[-1] if '_' in name else name for name in intent_names]
        
        sns.heatmap(
            similarity_matrix,
            xticklabels=display_names,
            yticklabels=display_names,
            annot=False,
            cmap='RdYlBu_r',
            center=0.5,
            square=True,
            linewidths=0.1
        )
        
        plt.title('Intent Semantic Similarity Heatmap', fontsize=16, fontweight='bold')
        plt.xlabel('Intent Categories', fontsize=12)
        plt.ylabel('Intent Categories', fontsize=12)
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        plt.savefig(f"{output_dir}/intent_similarity_heatmap.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. Intentå†…èšæ€§åˆ†æå›¾
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        # èšç±»æ•°é‡åˆ†å¸ƒ
        cluster_counts = [result['n_clusters'] for result in cohesion_results.values()]
        ax1.hist(cluster_counts, bins=range(0, max(cluster_counts) + 2), alpha=0.7, color='skyblue', edgecolor='black')
        ax1.set_title('Distribution of Clusters per Intent', fontweight='bold')
        ax1.set_xlabel('Number of Clusters')
        ax1.set_ylabel('Number of Intents')
        ax1.grid(True, alpha=0.3)
        
        # å™ªå£°æ¯”ä¾‹åˆ†å¸ƒ
        noise_ratios = [result['noise_ratio'] for result in cohesion_results.values()]
        ax2.hist(noise_ratios, bins=20, alpha=0.7, color='lightcoral', edgecolor='black')
        ax2.set_title('Distribution of Noise Ratios', fontweight='bold')
        ax2.set_xlabel('Noise Ratio')
        ax2.set_ylabel('Number of Intents')
        ax2.grid(True, alpha=0.3)
        
        # è½®å»“ç³»æ•°åˆ†å¸ƒ
        silhouette_scores = [result['silhouette_score'] for result in cohesion_results.values() 
                           if result['silhouette_score'] > 0]
        if silhouette_scores:
            ax3.hist(silhouette_scores, bins=20, alpha=0.7, color='lightgreen', edgecolor='black')
        ax3.set_title('Distribution of Silhouette Scores', fontweight='bold')
        ax3.set_xlabel('Silhouette Score')
        ax3.set_ylabel('Number of Intents')
        ax3.grid(True, alpha=0.3)
        
        # æ ·æœ¬æ•°é‡åˆ†å¸ƒ
        sample_counts = [result['n_samples'] for result in cohesion_results.values()]
        ax4.hist(sample_counts, bins=20, alpha=0.7, color='gold', edgecolor='black')
        ax4.set_title('Distribution of Sample Counts per Intent', fontweight='bold')
        ax4.set_xlabel('Number of Samples')
        ax4.set_ylabel('Number of Intents')
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/intent_cohesion_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # 3. 2Då¯è§†åŒ–æ•£ç‚¹å›¾ï¼ˆä½¿ç”¨UMAPé™ç»´ï¼‰
        logger.info("åˆ›å»º2Då¯è§†åŒ–æ•£ç‚¹å›¾")
        
        # æ”¶é›†æ‰€æœ‰åµŒå…¥å‘é‡å’Œæ ‡ç­¾
        all_embeddings = []
        all_labels = []
        intent_colors = {}
        
        # ä¸ºæ¯ä¸ªIntentåˆ†é…é¢œè‰²
        colors = plt.cm.Set3(np.linspace(0, 1, len(cohesion_results)))
        
        for idx, (intent, result) in enumerate(cohesion_results.items()):
            embeddings = result['embeddings']
            all_embeddings.append(embeddings)
            all_labels.extend([intent] * len(embeddings))
            intent_colors[intent] = colors[idx]
        
        if all_embeddings:
            all_embeddings = np.vstack(all_embeddings)
            
            # ä½¿ç”¨UMAPè¿›è¡Œé™ç»´
            reducer = UMAP(n_components=2, random_state=42, metric='cosine')
            embedding_2d = reducer.fit_transform(all_embeddings)
            
            # åˆ›å»ºæ•£ç‚¹å›¾
            plt.figure(figsize=(14, 10))
            
            for intent in intent_colors:
                mask = np.array(all_labels) == intent
                if np.any(mask):
                    plt.scatter(
                        embedding_2d[mask, 0],
                        embedding_2d[mask, 1],
                        c=[intent_colors[intent]],
                        label=intent.split('_')[-1] if '_' in intent else intent,
                        alpha=0.6,
                        s=30
                    )
            
            plt.title('2D Visualization of Intent Utterances (UMAP)', fontsize=16, fontweight='bold')
            plt.xlabel('UMAP Dimension 1', fontsize=12)
            plt.ylabel('UMAP Dimension 2', fontsize=12)
            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig(f"{output_dir}/intent_2d_visualization.png", dpi=300, bbox_inches='tight')
            plt.close()
            
            # 4. 3Då¯è§†åŒ–æ•£ç‚¹å›¾ï¼ˆä½¿ç”¨UMAPé™ç»´ï¼‰
            logger.info("åˆ›å»º3Då¯è§†åŒ–æ•£ç‚¹å›¾")
            
            # ä½¿ç”¨UMAPè¿›è¡Œ3Dé™ç»´
            reducer_3d = UMAP(n_components=3, random_state=42, metric='cosine')
            embedding_3d = reducer_3d.fit_transform(all_embeddings)

            # åˆ›å»º3Dæ•£ç‚¹å›¾
            fig = plt.figure(figsize=(16, 12))
            ax = fig.add_subplot(111, projection='3d')

            for intent in intent_colors:
                mask = np.array(all_labels) == intent
                if np.any(mask):
                    ax.scatter(
                        embedding_3d[mask, 0],
                        embedding_3d[mask, 1],
                        embedding_3d[mask, 2],
                        c=[intent_colors[intent]],
                        label=intent.split('_')[-1] if '_' in intent else intent,
                        alpha=0.6,
                        s=30
                    )
            
            ax.set_title('3D Visualization of Intent Utterances (UMAP)', fontsize=16, fontweight='bold')
            ax.set_xlabel('UMAP Dimension 1', fontsize=12)
            ax.set_ylabel('UMAP Dimension 2', fontsize=12)
            ax.set_zlabel('UMAP Dimension 3', fontsize=12)
            ax.legend(bbox_to_anchor=(1.1, 1), loc='upper left', fontsize=8)
            plt.tight_layout()
            plt.savefig(f"{output_dir}/intent_3d_visualization.png", dpi=300, bbox_inches='tight')
            plt.close()
        
        logger.info(f"å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ° {output_dir} ç›®å½•")
    
    def generate_report(self, 
                       similarity_results: pd.DataFrame,
                       cohesion_results: Dict[str, Dict],
                       output_dir: str = "output") -> None:
        """
        ç”Ÿæˆåˆ†ææŠ¥å‘Š
        
        Args:
            similarity_results: ç›¸ä¼¼åº¦åˆ†æç»“æœ
            cohesion_results: å†…èšæ€§åˆ†æç»“æœ
            output_dir: è¾“å‡ºç›®å½•
        """
        logger.info("ç”Ÿæˆåˆ†ææŠ¥å‘Š")
        
        os.makedirs(output_dir, exist_ok=True)
        
        report_lines = [
            "# Azure CLUæ•°æ®è´¨é‡åˆ†ææŠ¥å‘Š",
            "",
            f"åˆ†ææ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "## 1. æ•°æ®æ¦‚è§ˆ",
            f"- Intentæ€»æ•°: {len(self.intents)}",
            f"- Utteranceæ€»æ•°: {len(self.utterances)}",
            f"- å¹³å‡æ¯ä¸ªIntentçš„Utteranceæ•°é‡: {len(self.utterances) / len(self.intents):.1f}",
            "",
            "## 2. Intenté—´ä½è€¦åˆæ€§åˆ†æ",
            ""
        ]
        
        if not similarity_results.empty:
            report_lines.extend([
                f"å‘ç° {len(similarity_results)} å¯¹é«˜ç›¸ä¼¼åº¦Intentï¼ˆç›¸ä¼¼åº¦ > 0.8ï¼‰:",
                ""
            ])
            
            for _, row in similarity_results.head(10).iterrows():
                report_lines.append(
                    f"- {row['Intent1']} â†” {row['Intent2']} (ç›¸ä¼¼åº¦: {row['Similarity']:.3f})"
                )
            
            if len(similarity_results) > 10:
                report_lines.append(f"... è¿˜æœ‰ {len(similarity_results) - 10} å¯¹")
        else:
            report_lines.append("âœ… æœªå‘ç°é«˜ç›¸ä¼¼åº¦Intentå¯¹ï¼Œè¯­ä¹‰åŒºåˆ†åº¦è‰¯å¥½")
        
        report_lines.extend([
            "",
            "## 3. Intentå†…é«˜å†…èšæ€§åˆ†æ",
            ""
        ])
        
        # ç»Ÿè®¡å†…èšæ€§é—®é¢˜
        multi_cluster_intents = []
        high_noise_intents = []
        low_silhouette_intents = []
        
        for intent, result in cohesion_results.items():
            if result['n_clusters'] > 1:
                multi_cluster_intents.append((intent, result['n_clusters']))
            
            if result['noise_ratio'] > 0.2:  # å™ªå£°æ¯”ä¾‹è¶…è¿‡20%
                high_noise_intents.append((intent, result['noise_ratio']))
            
            if result['silhouette_score'] > 0 and result['silhouette_score'] < 0.5:
                low_silhouette_intents.append((intent, result['silhouette_score']))
        
        if multi_cluster_intents:
            report_lines.extend([
                f"### 3.1 å¤šèšç±»Intent ({len(multi_cluster_intents)}ä¸ª)",
                "ä»¥ä¸‹Intentå†…éƒ¨å¯èƒ½è¯­ä¹‰è¿‡å®½ï¼Œå»ºè®®æ‹†åˆ†:",
                ""
            ])
            
            for intent, n_clusters in sorted(multi_cluster_intents, key=lambda x: x[1], reverse=True)[:10]:
                report_lines.append(f"- {intent} ({n_clusters}ä¸ªèšç±»)")
        
        if high_noise_intents:
            report_lines.extend([
                "",
                f"### 3.2 é«˜å™ªå£°Intent ({len(high_noise_intents)}ä¸ª)",
                "ä»¥ä¸‹Intentå­˜åœ¨è¾ƒå¤šå¼‚å¸¸æ ·æœ¬ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®è´¨é‡:",
                ""
            ])
            
            for intent, noise_ratio in sorted(high_noise_intents, key=lambda x: x[1], reverse=True)[:10]:
                report_lines.append(f"- {intent} (å™ªå£°æ¯”ä¾‹: {noise_ratio:.1%})")
        
        # ä¿å­˜æŠ¥å‘Š
        with open(f"{output_dir}/analysis_report.md", 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        similarity_results.to_csv(f"{output_dir}/high_similarity_intents.csv", index=False)
        
        cohesion_df = pd.DataFrame([
            {
                'Intent': intent,
                'Samples': result['n_samples'],
                'Clusters': result['n_clusters'],
                'Noise': result['n_noise'],
                'NoiseRatio': result['noise_ratio'],
                'SilhouetteScore': result['silhouette_score']
            }
            for intent, result in cohesion_results.items()
        ])
        
        cohesion_df.to_csv(f"{output_dir}/intent_cohesion_metrics.csv", index=False)
        
        logger.info(f"åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ° {output_dir} ç›®å½•")
    
    def run_full_analysis(self, json_file_path: str, output_dir: str = "output") -> None:
        """
        è¿è¡Œå®Œæ•´çš„æ•°æ®è´¨é‡åˆ†ææµç¨‹
        
        Args:
            json_file_path: è¾“å…¥JSONæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
        """
        logger.info("å¼€å§‹å®Œæ•´çš„CLUæ•°æ®è´¨é‡åˆ†æ")
        
        # 1. åŠ è½½æ•°æ®
        self.load_data(json_file_path)
        
        # 2. ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰åµŒå…¥å‘é‡
        self._generate_all_embeddings()
        
        # 3. Intenté—´ä½è€¦åˆæ€§åˆ†æ
        similarity_matrix, similarity_results, heatmap_intent_names = self.analyze_inter_intent_coupling()
        
        # 4. Intentå†…é«˜å†…èšæ€§åˆ†æ
        cohesion_results = self.analyze_intra_intent_cohesion()
        
        # 5. åˆ›å»ºå¯è§†åŒ–
        self.create_visualizations(
            similarity_matrix, 
            heatmap_intent_names, 
            cohesion_results, 
            output_dir
        )
        
        # 6. ç”ŸæˆæŠ¥å‘Š
        self.generate_report(similarity_results, cohesion_results, output_dir)
        
        logger.info("CLUæ•°æ®è´¨é‡åˆ†æå®Œæˆ")


def main():
    """ä¸»å‡½æ•°"""
    # Azure OpenAIé…ç½®
    API_KEY = "7218515241f04d98b3b5d9869a25b91f"  # è¯·æ›¿æ¢ä¸ºå®é™…APIå¯†é’¥
    AZURE_ENDPOINT = "https://nnitasia-openai-01-ins.openai.azure.com/"
    DEPLOYMENT_NAME = "NNIT-Ada-3-large"
    
    # æ•°æ®æ–‡ä»¶è·¯å¾„
    JSON_FILE_PATH = "data/IT_01_1.json"
    OUTPUT_DIR = "output"
    
    try:
        # åˆ›å»ºåˆ†æå™¨å®ä¾‹
        analyzer = CLUDataAnalyzer(
            api_key=API_KEY,
            azure_endpoint=AZURE_ENDPOINT,
            deployment_name=DEPLOYMENT_NAME
        )
        
        # è¿è¡Œå®Œæ•´åˆ†æ
        analyzer.run_full_analysis(JSON_FILE_PATH, OUTPUT_DIR)
        
        print("âœ… CLUæ•°æ®è´¨é‡åˆ†æå®Œæˆï¼")
        print(f"ğŸ“Š ç»“æœå·²ä¿å­˜åˆ° {OUTPUT_DIR} ç›®å½•")
        print("ğŸ“ˆ è¯·æŸ¥çœ‹ç”Ÿæˆçš„å¯è§†åŒ–å›¾è¡¨å’Œåˆ†ææŠ¥å‘Š")
        
    except Exception as e:
        logger.error(f"åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        raise


if __name__ == "__main__":
    main()
